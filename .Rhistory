carcompanyvolvo + enginetypedohcv, data = train)
## Now check adjusted R value on new model
summary(model_9)
## R-squared:  0.9738 and Adjusted R-squared:  0.9662 , p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# Check VIF of new model for verifying next insignificant variable
vif(model_9)
# Note that the variables curbweight,enginesize,carwidth,cylindernumbersix,cylindernumberfour,carcompanytoyota,stroke,carcompanyhonda,carcompanybuick,drivewheelfwd,carcompanynissan,
# enginetypeohcf,carcompanymazda,cylindernumberfive,carcompanymitsubishi,carcompanyvolkswagen,carcompanyvolvo,enginetypel,carcompanyplymouth,carcompanysaab,carcompanyjaguar,
# enginelocation,carcompanydodge,aspiration have the highest VIFs,but cannot be removed since these are highly significant(p-value< 0.05).
# Also, notice that the variables curbweight and enginesize still have the highest VIFs with very high significance since the beginning.
# So, it will be a good idea to check their correlation as they might be highly correlated.
# Check the correlation of the variables curbweight and enginesize.Use cor().
cor(train$curbweight,train$enginesize)
# Note that the correlation is ~ 86%, indicating that the variables are highly correlated.
# So, remove the variable with lower significance level out of the two and build model_10.(Remove enginesize)
################## Model 10 #####################
model_10 <- lm(formula = price ~ aspiration + enginelocation +
carwidth + curbweight + stroke +
compressionratio + carbodyhatchback +
carbodywagon + drivewheelfwd + enginetypel +
enginetypeohcf + cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw +
carcompanybuick + carcompanydodge + carcompanyhonda + carcompanyisuzu +
carcompanyjaguar + carcompanymazda + carcompanymercury +
carcompanymitsubishi + carcompanynissan + carcompanyplymouth +
carcompanyrenault + carcompanysaab + carcompanytoyota + carcompanyvolkswagen +
carcompanyvolvo + enginetypedohcv, data = train)
## Now check adjusted R value on new model
summary(model_10)
## R-squared:  0.9716 and Adjusted R-squared:  0.9636 , p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# Check VIF of new model for verifying next insignificant variable
vif(model_10)
# Note that the variables curbweight,carwidth,cylindernumberfour,carcompanytoyota,cylindernumbersix have the highest VIFs,
# but cannot be removed since these are highly significant(p-value< 0.05).
# So, the variable with next highest VIF is stroke. It is insignificant as well(p-value> 0.05). Hence, stroke can be removed from the model.
################## Model 11 #####################
model_11 <- lm(formula = price ~ aspiration + enginelocation +
carwidth + curbweight + compressionratio + carbodyhatchback +
carbodywagon + drivewheelfwd + enginetypel +
enginetypeohcf + cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw +
carcompanybuick + carcompanydodge + carcompanyhonda + carcompanyisuzu +
carcompanyjaguar + carcompanymazda + carcompanymercury +
carcompanymitsubishi + carcompanynissan + carcompanyplymouth +
carcompanyrenault + carcompanysaab + carcompanytoyota + carcompanyvolkswagen +
carcompanyvolvo + enginetypedohcv, data = train)
## Now check adjusted R value on new model
summary(model_11)
## R-squared:  0.9706 and Adjusted R-squared:  0.9627 , p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# Check VIF of new model for verifying next insignificant variable
vif(model_11)
# Note that the variables curbweight,carwidth,cylindernumberfour,carcompanytoyota,drivewheelfwd,carcompanynissan,carcompanyhonda,carcompanymazda,enginetypeohcf,carcompanybuick,cylindernumberfive,
# carcompanymitsubishi,carcompanyvolkswagen have the highest VIFs,but cannot be removed since these are highly significant(p-value< 0.05).
# Also, notice that the variables curbweight and carwidth still have the highest VIFs with very high significance since the beginning.
# So, it will be a good idea to check their correlation as they might be highly correlated.
# Check the correlation of the variables curbweight and carwidth.Use cor().
cor(train$curbweight,train$carwidth)
# Note that the correlation is ~ 87%, indicating that the variables are highly correlated.
# So, remove the variable with lower significance level out of the two and build model_12.(Remove carwidth)
################## Model 12 #####################
model_12 <- lm(formula = price ~ aspiration + enginelocation +
curbweight + compressionratio + carbodyhatchback +
carbodywagon + drivewheelfwd + enginetypel +
enginetypeohcf + cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw +
carcompanybuick + carcompanydodge + carcompanyhonda + carcompanyisuzu +
carcompanyjaguar + carcompanymazda + carcompanymercury +
carcompanymitsubishi + carcompanynissan + carcompanyplymouth +
carcompanyrenault + carcompanysaab + carcompanytoyota + carcompanyvolkswagen +
carcompanyvolvo + enginetypedohcv, data = train)
## Now check adjusted R value on new model
summary(model_12)
## R-squared:  0.9657 and Adjusted R-squared:  0.9569 , p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# Check VIF of new model for verifying next insignificant variable
vif(model_12)
#Checking variable which have vif>2 and are insignificant
# So, the variable with next highest VIF is carcompanyvolvo and  It is insignificant as well(p-value> 0.05). Hence, stroke can be removed from the model.
################## Model 13 #####################
model_13 <- lm(formula = price ~ aspiration + enginelocation +
curbweight + compressionratio + carbodyhatchback +
carbodywagon + drivewheelfwd + enginetypel +
enginetypeohcf + cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw +
carcompanybuick + carcompanydodge + carcompanyhonda + carcompanyisuzu +
carcompanyjaguar + carcompanymazda + carcompanymercury +
carcompanymitsubishi + carcompanynissan + carcompanyplymouth +
carcompanyrenault + carcompanysaab + carcompanytoyota + carcompanyvolkswagen +
enginetypedohcv, data = train)
## Now check adjusted R value on new model
summary(model_13)
## R-squared:  0.9651 and Adjusted R-squared:  0.9565 , p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# Check VIF of new model for verifying next insignificant variable
vif(model_13)
# All varibales having vif>2 are significant.Checked correlation as well among varibales having high VIF but didn't find high correlation on them
# So, now we will remove variables on the basis of p-values (or significance).variable.
# Note that the variable aspiration has the highest p-value(or lowest significance). So, remove this variable. Use model_14<-lm().
################## Model 14 #####################
model_14 <- lm(formula = price ~ enginelocation +
curbweight + compressionratio + carbodyhatchback +
carbodywagon + drivewheelfwd + enginetypel +
enginetypeohcf + cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw +
carcompanybuick + carcompanydodge + carcompanyhonda + carcompanyisuzu +
carcompanyjaguar + carcompanymazda + carcompanymercury +
carcompanymitsubishi + carcompanynissan + carcompanyplymouth +
carcompanyrenault + carcompanysaab + carcompanytoyota + carcompanyvolkswagen +
enginetypedohcv, data = train)
## Now check adjusted R value on new model
summary(model_14)
## R-squared:  0.964 and Adjusted R-squared:  0.9555 , p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# Check VIF of new model for verifying next insignificant variable
vif(model_14)
# All varibales having vif>2 are significant.Checked correlation as well among varibales having high VIF but didn't find high correlation on them
# So, now we will remove variables on the basis of p-values (or significance).variable.
# Note that the variable carbodyhatchback has the highest p-value(or lowest significance). So, remove this variable. Use model_15<-lm().
################## Model 15 #####################
model_15 <- lm(formula = price ~ enginelocation +
curbweight + compressionratio +
carbodywagon + drivewheelfwd + enginetypel +
enginetypeohcf + cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw +
carcompanybuick + carcompanydodge + carcompanyhonda + carcompanyisuzu +
carcompanyjaguar + carcompanymazda + carcompanymercury +
carcompanymitsubishi + carcompanynissan + carcompanyplymouth +
carcompanyrenault + carcompanysaab + carcompanytoyota + carcompanyvolkswagen +
enginetypedohcv, data = train)
## Now check adjusted R value on new model
summary(model_15)
## R-squared:  0.964 and Adjusted R-squared:  0.9559 , p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# Check again VIF of new model for verifying next insignificant variable.
vif(model_15)
# All varibales having vif>2 are significant.Checked correlation as well among varibales having high VIF but didn't find high correlation on them
# So, now we will remove variables on the basis of p-values (or significance).variable.
# Note that the variable carcompanyjaguar has the highest p-value(or lowest significance). So, remove this variable. Use model_16<-lm().
################## Model 16 #####################
model_16 <- lm(formula = price ~ enginelocation +
curbweight + compressionratio +
carbodywagon + drivewheelfwd + enginetypel +
enginetypeohcf + cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw +
carcompanybuick + carcompanydodge + carcompanyhonda + carcompanyisuzu +
carcompanymazda + carcompanymercury +
carcompanymitsubishi + carcompanynissan + carcompanyplymouth +
carcompanyrenault + carcompanysaab + carcompanytoyota + carcompanyvolkswagen +
enginetypedohcv, data = train)
## Now check adjusted R value on new model
summary(model_16)
## R-squared:  0.9628 and Adjusted R-squared:  0.9549, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# Check again VIF of new model for verifying next insignificant variable.
vif(model_16)
# All varibales having vif>2 are significant.Checked correlation as well among varibales having high VIF but didn't find high correlation on them
# So, now we will remove variables on the basis of p-values (or significance).variable.
# Note that the variable carcompanymercury has the highest p-value(or lowest significance). So, remove this variable. Use model_17<-lm().
################## Model 17 #####################
model_17 <- lm(formula = price ~ enginelocation +
curbweight + compressionratio +
carbodywagon + drivewheelfwd + enginetypel +
enginetypeohcf + cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw +
carcompanybuick + carcompanydodge + carcompanyhonda + carcompanyisuzu +
carcompanymazda +
carcompanymitsubishi + carcompanynissan + carcompanyplymouth +
carcompanyrenault + carcompanysaab + carcompanytoyota + carcompanyvolkswagen +
enginetypedohcv, data = train)
## Now check adjusted R value on new model
summary(model_17)
## R-squared:  0.9625 and Adjusted R-squared:  0.9548, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# Check again VIF of new model for verifying next insignificant variable.
vif(model_17)
# All varibales having vif>2 are significant.Checked correlation as well among varibales having high VIF but didn't find high correlation on them.
# So, now we will remove variables on the basis of p-values (or significance).variable.
# Note that the variable enginetypedohcv has the highest p-value(or lowest significance). So, remove this variable. Use model_18<-lm().
################## Model 18 #####################
model_18 <- lm(formula = price ~ enginelocation +
curbweight + compressionratio +
carbodywagon + drivewheelfwd + enginetypel +
enginetypeohcf + cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw +
carcompanybuick + carcompanydodge + carcompanyhonda + carcompanyisuzu +
carcompanymazda +
carcompanymitsubishi + carcompanynissan + carcompanyplymouth +
carcompanyrenault + carcompanysaab + carcompanytoyota + carcompanyvolkswagen,
data = train)
## Now check adjusted R value on new model
summary(model_18)
## R-squared:  0.9615 and Adjusted R-squared:  0.9541, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# Check again VIF of new model for verifying next insignificant variable.
vif(model_18)
# All varibales having vif>2 are significant.Checked correlation as well among varibales having high VIF but didn't find high correlation on them.
# So, now we will remove variables on the basis of p-values (or significance).variable.
# Note that the variable compressionratio has the highest p-value(or lowest significance). So, remove this variable. Use model_19<-lm().
################## Model 19 #####################
model_19 <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + drivewheelfwd + enginetypel +
enginetypeohcf + cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw +
carcompanybuick + carcompanydodge + carcompanyhonda + carcompanyisuzu +
carcompanymazda +
carcompanymitsubishi + carcompanynissan + carcompanyplymouth +
carcompanyrenault + carcompanysaab + carcompanytoyota + carcompanyvolkswagen,
data = train)
## Now check adjusted R value on new model
summary(model_19)
## R-squared:  0.9604 and Adjusted R-squared:  0.9531, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# So, now we will remove variables on the basis of p-values (or significance).variable.
# The variable carcompanyisuzu has the highest p-value(or lowest significance). So, remove this variable. Use model_20<-lm().
################## Model 20 #####################
model_20 <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + drivewheelfwd + enginetypel +
enginetypeohcf + cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw + carcompanybuick +
carcompanydodge + carcompanyhonda + carcompanymazda +
carcompanymitsubishi + carcompanynissan + carcompanyplymouth +
carcompanyrenault + carcompanysaab + carcompanytoyota + carcompanyvolkswagen,
data = train)
## Now check adjusted R value on new model
summary(model_20)
## R-squared:  0.9574 and Adjusted R-squared:  0.95, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# So, now we will remove variables on the basis of p-values (or significance).variable.
# Note that the variable carcompanysaab has the highest p-value(or lowest significance). So, remove this variable. Use model_21<-lm().
################## Model 21 #####################
model_21 <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + drivewheelfwd + enginetypel +
enginetypeohcf + cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw + carcompanybuick +
carcompanydodge + carcompanyhonda + carcompanymazda +
carcompanymitsubishi + carcompanynissan + carcompanyplymouth +
carcompanyrenault + carcompanytoyota + carcompanyvolkswagen,
data = train)
## Now check adjusted R value on new model
summary(model_21)
## R-squared:  0.9544 and Adjusted R-squared:  0.9469, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# So, now we will remove variables on the basis of p-values (or significance).variable.
# Note that the variable carcompanyhonda has the highest p-value(or lowest significance). So, remove this variable. Use model_22<-lm().
################## Model 22 #####################
model_22 <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + drivewheelfwd + enginetypel +
enginetypeohcf + cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw + carcompanybuick +
carcompanydodge + carcompanymazda +
carcompanymitsubishi + carcompanynissan + carcompanyplymouth +
carcompanyrenault + carcompanytoyota + carcompanyvolkswagen,
data = train)
## Now check adjusted R value on new model
summary(model_22)
## R-squared:  0.9512 and Adjusted R-squared:  0.9437, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# So, now we will remove variables on the basis of p-values (or significance).variable.
# Note that the variable carcompanynissan has the highest p-value(or lowest significance). So, remove this variable. Use model_23<-lm().
################## Model 23 #####################
model_23 <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + drivewheelfwd + enginetypel +
enginetypeohcf + cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw + carcompanybuick +
carcompanydodge + carcompanymazda +
carcompanymitsubishi + carcompanyplymouth +
carcompanyrenault + carcompanytoyota + carcompanyvolkswagen,
data = train)
## Now check adjusted R value on new model
summary(model_23)
## R-squared:  0.9498 and Adjusted R-squared:  0.9425, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# So, now we will remove variables on the basis of p-values (or significance).variable.
# Note that the variable carcompanyplymouth has the highest p-value(or lowest significance). So, remove this variable. Use model_24<-lm().
################## Model 24 #####################
model_24 <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + drivewheelfwd + enginetypel +
enginetypeohcf + cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw + carcompanybuick +
carcompanydodge + carcompanymazda + carcompanymitsubishi +
carcompanyrenault + carcompanytoyota + carcompanyvolkswagen,
data = train)
## Now check adjusted R value on new model
summary(model_24)
## R-squared:  0.9478 and Adjusted R-squared:  0.9407, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# So, now we will remove variables on the basis of p-values (or significance).variable.
# Note that the variable enginetypeohcf has the highest p-value(or lowest significance). So, remove this variable. Use model_25<-lm().
################## Model 25 #####################
model_25 <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + drivewheelfwd + enginetypel +
cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw + carcompanybuick +
carcompanydodge + carcompanymazda + carcompanymitsubishi +
carcompanyrenault + carcompanytoyota + carcompanyvolkswagen,
data = train)
## Now check adjusted R value on new model
summary(model_25)
## R-squared:  0.9462 and Adjusted R-squared:  0.9394, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# So, now we will remove variables on the basis of p-values (or significance).variable.
# Note that the variable carcompanyrenault has the highest p-value(or lowest significance). So, remove this variable. Use model_26<-lm().
################## Model 26 #####################
model_26 <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + drivewheelfwd + enginetypel +
cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw + carcompanybuick +
carcompanydodge + carcompanymazda + carcompanymitsubishi +
carcompanytoyota + carcompanyvolkswagen,
data = train)
## Now check adjusted R value on new model
summary(model_26)
## R-squared:  0.9446 and Adjusted R-squared:  0.9381, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# So, now we will remove variables on the basis of p-values (or significance).variable.
# Note that the variable carcompanydodge has the highest p-value(or lowest significance). So, remove this variable. Use model_27<-lm().
################## Model 27 #####################
model_27 <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + drivewheelfwd + enginetypel +
cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw + carcompanybuick +
carcompanymazda + carcompanymitsubishi +
carcompanytoyota + carcompanyvolkswagen,
data = train)
## Now check adjusted R value on new model
summary(model_27)
## R-squared:  0.9432 and Adjusted R-squared:  0.937, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# So, now we will remove variables on the basis of p-values (or significance).variable.
# Note that the variable carcompanyvolkswagen has the highest p-value(or lowest significance). So, remove this variable. Use model_28<-lm().
################## Model 28 #####################
model_28 <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + drivewheelfwd + enginetypel +
cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw + carcompanybuick +
carcompanymazda + carcompanymitsubishi +
carcompanytoyota, data = train)
## Now check adjusted R value on new model
summary(model_28)
## R-squared:  0.9432 and Adjusted R-squared:  0.937, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# So, now we will remove variables on the basis of p-values (or significance).variable.
# Note that the variable drivewheelfwd has the highest p-value(or lowest significance). So, remove this variable. Use model_29<-lm().
################## Model 29 #####################
model_29 <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + enginetypel +
cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw + carcompanybuick +
carcompanymazda + carcompanymitsubishi +
carcompanytoyota, data = train)
## Now check adjusted R value on new model
summary(model_29)
## R-squared:  0.9393 and Adjusted R-squared:  0.9337, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# So, now we will remove variables on the basis of p-values (or significance).variable.
# Note that the variable carcompanymitsubishi has the highest p-value(or lowest significance). So, remove this variable. Use model_30<-lm().
################## Model 30 #####################
model_30 <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + enginetypel +
cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw + carcompanybuick +
carcompanymazda + carcompanytoyota, data = train)
## Now check adjusted R value on new model
summary(model_30)
## R-squared:  0.9361 and Adjusted R-squared:  0.9308, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# So, now we will remove variables on the basis of p-values (or significance).variable.
# Note that the variable carcompanymazda has the highest p-value(or lowest significance). So, remove this variable. Use model_31<-lm().
################## Model 31 #####################
model_31 <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + enginetypel +
cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw + carcompanybuick +
carcompanytoyota, data = train)
## Now check adjusted R value on new model
summary(model_31)
## R-squared:  0.9336 and Adjusted R-squared:  0.9285, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# Now all variables have well significance and less p-value.
#Lets remove a varibale(carcompanytoyota) which have high p-value () and have well significance and create a new model_32
################## Model 32 #####################
model_32 <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + enginetypel +
cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw + carcompanybuick, data = train)
## Now check adjusted R value on new model
summary(model_32)
## R-squared:  0.9278 and Adjusted R-squared:  0.9229, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
# Now all variables have well significance and less p-value.
#Lets remove a varibale(enginetypel) which have high p-value () and have well significance and create a new model_33
################## Model 33 #####################
model_33 <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + cylindernumberfive + cylindernumberfour +
cylindernumbersix + carcompanybmw + carcompanybuick, data = train)
## Now check adjusted R value on new model
summary(model_33)
## R-squared:  0.9206 and Adjusted R-squared:  0.9159, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
#Lets remove a varibale(cylindernumbersix) which have high p-value () and have less significance and create a new model_34
################## Model 34 #####################
model_34 <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + cylindernumberfive + cylindernumberfour +
carcompanybmw + carcompanybuick, data = train)
## Now check adjusted R value on new model
summary(model_34)
## R-squared:  0.9143 and Adjusted R-squared:  0.9099, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
#Lets remove a varibale(cylindernumberfive) which have high p-value () and have less significance and create a new model_35
################## Model 35 #####################
model_35 <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + cylindernumberfour +
carcompanybmw + carcompanybuick, data = train)
## Now check adjusted R value on new model
summary(model_35)
## R-squared:  0.9084 and Adjusted R-squared:  0.9043, p-value: < 2.2e-16
#There is not much reduction on adjusted R value so we are good to remove this variable
#Lets remove a varibale(cylindernumberfour) which have high p-value () and have well significance and create a new model_36
################## Model 36 #####################
model_36 <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + carcompanybmw + carcompanybuick,
data = train)
## Now check adjusted R value on new model
summary(model_36)
## R-squared:  0.8978 and Adjusted R-squared:  0.894, p-value: < 2.2e-16
#There is not much  reduction on adjusted R value so we are good to remove this variable
# All varibales are significant and p-value is less so this model could be best fit model but
# lets remove few more variable and find the impact before we decide all variables are best suited for model
# Let me remove carbodywagon since it has estimated and t value negative and create model_37
################## Model 37 #####################
model_37 <- lm(formula = price ~ enginelocation +
curbweight + carcompanybmw + carcompanybuick,
data = train)
## Now check adjusted R value on new model
summary(model_37)
## R-squared:  0.8816 and Adjusted R-squared:  0.8782, p-value: < 2.2e-16
#There is not much  reduction on adjusted R value so we are good to remove this variable
# All varibales are significant and p-value is less so this model could be best fit model but
# lets remove few more variable and find the impact on Adjuested R value before decide all variables are best suited for model
# Let me remove available varaibles on model one by one and check adjuested R value
# Adjusted R-squared value has decreased abruptly when we remove available variable from the model so this is the final model for prediction
############# Here we can say, Final model is model_37 but it has not confimed unless we check on test data ###################################
##############  Model Evaluation ###################
# Now, use model_37 to predict the price value for test data.
############# Prediction 1 ##################
Predict_1<-predict(model_37, test[, -10])
# In order to check, how accurate are the predictions of the model,
# we need to find the r-squared value between the predicted and actual values of price.
# R-squared is defined as the square of correlation between actual and predicted values of the variable.
(cor(test$price, Predict_1))^2 ##0.8234591
# Now r-squared value for the model is 88% while the r-squared value for the test dataset is 82%.
# Generally, a deviation of +(-) 5% in the R-squared value of the model for the test data is acceptable.
# However, here deviation is (>5%) so we need to go back to upper model where we removed the variable.
############# Prediction 2 using model_36 ##################
# Now, use model_36 to predict the price value for test data.
Predict_2<-predict(model_36, test[, -10])
(cor(test$price, Predict_2))^2 ####0.8382702
summary(model_36)  # 89%
# Now r-squared value for the model is 89% while the r-squared value for the test dataset is 84%.
# Generally, a deviation of +(-) 5% in the R-squared value of the model for the test data is acceptable.
# Here deviation is (~5%) so we can confirm , this would be the final model and variables on this model will be final varibales
################## FINAL MODEL IS model_36 ####################
## FINAL vARIABLES are - enginelocation ,curbweight, carbodywagon , carcompanybmw ,carcompanybuick
##### Lets Evalutae our model on whole data set ######
# We can check if a model works well for whole dataset data in many different ways.Here we are using graphical representation to know about. We pay great attention to regression results, such as slope coefficients, p-values, or R2 that tell us how well a model represents on given data.
# Residuals could show how poorly a model represents data. Residuals are leftover of the outcome variable after fitting a model (predictors) to data and they could reveal unexplained patterns in the data by the fitted model.
# Using this information, we check, if linear regression assumptions are met or improvement is required.
# Here we are running analysis on whole data set  geelyauto_7
geelyautofit = lm(price ~ enginelocation+curbweight+carbodywagon+carcompanybmw+carcompanybuick, geelyauto_7)
par(mfrow=c(2,2))
plot(geelyautofit)
# Here is our observation on different generated graphs
# 1. Residuals vs Fitted :-We could find equally spread residuals around a linear line without distinct patterns, so it is a good indication that it does not have non-linear relationships
# 2. Normal Q-Q :- It's good residuals are lined well on the straight dashed line.
# 3. Scale-Location :- It's good we could see a horizontal line with equally (randomly) spread points.
# 4. Residuals vs Leverage :- The plot identified the influential observation on #17 and #75 rows where price could have outliers so Lets remove those remove and run our test again
#### Lets remove ourlier from dataset and generate "Residuals vs Leverage" graph again
# let me handle outlier on price  or remove these 2 rows from dataset and check slope coefficients, p-values, or R2. Here i am not using quatile function to handle ourlier but removing those rows from dataset
# On Price (Outliers)
#Using Box plot for identiying outliers
boxplot(geelyauto_7$price, main="Wheel base", sub=paste("Outlier rows: ", boxplot.stats(geelyauto_7$price)$out))
## Remove 17 and 75 rows from data set and check model,
geelyauto_7 <- geelyauto_7[-c(17, 75), ]
## Lets draw the plot again
geelyautofit = lm(price ~ enginelocation+curbweight+carbodywagon+carcompanybmw+carcompanybuick, geelyauto_7)
par(mfrow=c(2,2))
plot(geelyautofit)
####Check "Residuals vs Leverage" and its look fine now.
#### Lets check R and Adjusted R value
### Here created train1 and test1 data set from new dataset for predicting the values
set.seed(100)
trainindices= sample(1:nrow(geelyauto_7), 0.7*nrow(geelyauto_7))
train1 = geelyauto_7[trainindices,]
test1 = geelyauto_7[-trainindices,]
model_final <- lm(formula = price ~ enginelocation +
curbweight + carbodywagon + carcompanybmw + carcompanybuick,
data = train1)
summary(model_final)
## R-squared:  0.8903 and Adjusted R-squared:  0.8863, p-value: < 2.2e-16
#There is not much  reduction on adjusted R value so we are good to remove those 2 rows from dataset
############# Prediction 3 using model_final ##################
# Now, use model_final to predict the price value for test data.
Predict_3<-predict(model_final, test[, -10])
(cor(test$price, Predict_3))^2 ####0.8238364
summary(model_final)  # 89%
# Now r-squared value for the model is 88% while the r-squared value for the test dataset is 82%.# Generally, a deviation of +(-) 5% in the R-squared value of the model for the test data is acceptable.
# Here deviation is more than (~5%) so  it is always a subjective call to take
################## FINAL MODEL IS model_36 or model_final ####################
## FINAL vARIABLES are - enginelocation ,curbweight, carbodywagon , carcompanybmw ,carcompanybuick
